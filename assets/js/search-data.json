{
  
    
        "post0": {
            "title": "SGD 实验2",
            "content": ". path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 . train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) dset = list(zip(train_x,train_y)) valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . x, y = dset[0] x.shape, y.shape . (torch.Size([784]), torch.Size([1])) . 这里把每一幅图28*28pixel变成一个vector，也就是[1x784]的一个tensor。label变成[1x1]的tensor . 利用的architecture是 (Weight*ImageTensor).sum()+bias。 . Tip: 这里的weight和ImageTensor是bitwise multiply，也就是每一个pixel有一个weight，得出来的tensor还是[1x784]的大小 . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() weights = init_params((28*28,1)) bias = init_params(1) . (train_x[0]*weights.T).sum() + bias . tensor([15.7077], grad_fn=&lt;AddBackward0&gt;) . 利用pytorch的GPU功能来计算&quot;@&quot;表示是矩阵乘法 . def linear1(xb): return xb@weights + bias preds = linear1(train_x) . 其实这里preds&gt;0.0就是loss function，这里的0是随意选取的，没有太大意义。 . corrects = (preds&gt;0.0).float() == train_y corrects.float().mean().item() . 0.5525975823402405 . weights[0] *= 1.0001 preds = linear1(train_x) ((preds&gt;0.0).float() == train_y).float().mean().item() . 0.5525975823402405 . 从SGD实验1里，我们知道我们的最终目的是得出合适的parameter，这里也就是weights和bias，来带入architecture做预测。要得出合适的parameter，就是要通过计算parameter的gradient,然后通过 parameter = parameter - (learning rate) * gradient来更新parameter。而gradient是通过loss function的derivative来求得的。但是有的loss function当parameter是0的时候，它的derivative也是0，比如说 loss = x^2。这样的话parameter就不会被更新。我们希望找到一种loss function，使得只要跟新一点点weight使得prediction变好一点，它的loss也会变小一点。 . Having defined a loss function, now is a good moment to recapitulate why we did this. After all, we already had a metric, which was overall accuracy. So why did we define a loss? . The key difference is that the metric is to drive human understanding and the loss is to drive automated learning. To drive automated learning, the loss must be a function that has a meaningful derivative. It can&#39;t have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level. This requirement means that sometimes it does not really reflect exactly what we are trying to achieve, but is rather a compromise between our real goal, and a function that can be optimized using its gradient. The loss function is calculated for each item in our dataset, and then at the end of an epoch the loss values are all averaged and the overall mean is reported for the epoch. . Metrics, on the other hand, are the numbers that we really care about. These are the values that are printed at the end of each epoch that tell us how our model is really doing. It is important that we learn to focus on these metrics, rather than the loss, when judging the performance of a model. . 下面这个loss function就符合要求，先把所有的prediction用sigmoid变到[0,1]内，然后直接计算距离的mean值。上面这段是直接从fast.ai的书里摘录出来的，对理解metric和loss function的区别很有帮助。 . weights = init_params((28*28,1)) bias = init_params(1) . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() # torch的function return torch.where(targets==1, 1-predictions, predictions).mean() . dl = DataLoader(dset, batch_size=256) valid_dl = DataLoader(valid_dset, batch_size=256) . 把所有的合并成一个function，计算preds，然后计算loss，然后通过loss计算gradient . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . 当calc_grad计算出gradient后，更新parameter。并重置gradient . Warning: 为何要重置gradient？ . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.707 . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8539 0.9081 0.9291 0.9365 0.9443 0.9482 0.954 0.9589 0.9609 0.9623 0.9633 0.9662 0.9667 0.9682 0.9682 0.9687 0.9687 0.9692 0.9702 0.9702 . pytorch&#25552;&#20379;&#20102;&#24456;&#22810;&#26377;&#29992;&#30340;function&#65292;&#20813;&#21435;&#20102;&#33258;&#24049;&#21435;&#21019;&#36896; . linear_model = nn.Linear(28*28,1) class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None opt = BasicOptim(linear_model.parameters(), lr) def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.8335 0.8438 0.9116 0.9341 0.9482 0.9565 0.9624 0.9658 0.9678 0.9692 0.9717 0.9736 0.9746 0.9761 0.9761 0.9775 0.978 0.9785 0.979 . fastbook也提供了一些wrapper，比如说SGD . dls = DataLoaders(dl, valid_dl) learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.637081 | 0.503588 | 0.495584 | 00:00 | . 1 | 0.566746 | 0.186157 | 0.844946 | 00:00 | . 2 | 0.206826 | 0.185936 | 0.832188 | 00:00 | . 3 | 0.089577 | 0.108501 | 0.910697 | 00:00 | . 4 | 0.046465 | 0.078758 | 0.932287 | 00:00 | . 5 | 0.029719 | 0.062852 | 0.946025 | 00:00 | . 6 | 0.022896 | 0.053008 | 0.954367 | 00:00 | . 7 | 0.019905 | 0.046501 | 0.962218 | 00:00 | . 8 | 0.018413 | 0.041952 | 0.965162 | 00:00 | . 9 | 0.017528 | 0.038611 | 0.967125 | 00:00 | .",
            "url": "https://jsqihui.github.io/ai/fast.ai/2020/12/16/SGD-2.html",
            "relUrl": "/fast.ai/2020/12/16/SGD-2.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "SGD 实验1",
            "content": ". 假设我们有一个问题就是推测出过山车的速度模型，已知数据是过山车在0到20每个时间点的速度 . time = torch.arange(0,20).float() speed = torch.randn(20)*3 + 0.75*(time-9.5)**2 + 1 plt.scatter(time,speed); . 推测出architecture可能是个二阶多项式，然后要确定其中的参数，parameters . def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . 有了architecture后，还要找出一个loss function，这个loss function来确定对某parameters，它的performance如何，loss function的值越小越好 . def mse(preds, targets): return ((preds-targets)**2).mean().sqrt() . 现在的任务就是要找出这个参数，第一步初始化参数 . params = torch.randn(3).requires_grad_() . 用一个函数显示出当前预测数据和实际数据的差别 . def show_preds(preds, ax=None): if ax is None: ax=plt.subplots()[1] ax.scatter(time, speed) ax.scatter(time, to_np(preds), color=&#39;red&#39;) ax.set_ylim(-300,100) . 下面函数的执行步骤 . 用当前的params做出预测 | 利用当前预测和loss function求出loss | loss.backward用来求出当前参数的gradient | 利用learning rate（lr）和当前的gradient来移动当前的params | 求出loss | lr = 1e-3 def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . for i in range(10): apply_step(params) . 49.00632095336914 30.988157272338867 25.448453903198242 25.400514602661133 25.399803161621094 25.3996639251709 25.399532318115234 25.39940071105957 25.399269104003906 25.399139404296875 . _,axs = plt.subplots(1,4,figsize=(12,3)) for ax in axs: show_preds(apply_step(params, False), ax) plt.tight_layout() . &#26377;&#28857;&#38590;&#20197;&#29702;&#35299;&#30340;&#37096;&#20998; . loss.backward用来求当前的parameter的gradient，首先要明确目的：需要调整parameter使得loss更低。也就是说对于函数loss_function(params)，调整params使得函数值更小。简单的方法就是对loss_function() 求微分，比如说 loss_function = x^2, 当前x是6，那么当前的loss = 6^2 = 36，如何调整当前的x使得loss变小呢。只要求微分，也就是 (loss_function) = 2x，然后用x减去微分方向上的一个小小的值（利用lr，learning rate），那么更新后的x就是 new_x = x - lr2x = 6 - 0.12*6 = 4.8。这样求出来的new_x，再次代入loss_function,得到loss = 4.8^2 就比之前的6^2更小了。 .",
            "url": "https://jsqihui.github.io/ai/fast.ai/2020/12/16/SGD-1.html",
            "relUrl": "/fast.ai/2020/12/16/SGD-1.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jsqihui.github.io/ai/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jsqihui.github.io/ai/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jsqihui.github.io/ai/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jsqihui.github.io/ai/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}